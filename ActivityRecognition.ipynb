{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ActivityRecognition.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shaheerr/Activity_Recognition_WiFi/blob/main/ActivityRecognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUt26uoZZKRW"
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYUZCUH8uA1-"
      },
      "source": [
        "Note place the dataset in the root of Google drive and create these 4 folders in google drive:  \n",
        "1.ActivityRecognition/  \n",
        "2.ActivityRecognition/input_files  \n",
        "3.ActivityRecognition/input_files_2  \n",
        "4.ActivityRecognition/keras_models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCZsboE2uAup"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRJYILFmDlbq"
      },
      "source": [
        "!tar xvzf \"./drive/My Drive/Dataset.tar.gz\" -C \"./drive/My Drive/WifiActivityRecognition/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTez3uUQm20z"
      },
      "source": [
        "#Copy from \"input_files\" folder in root to input_files folder in drive/My drive/\n",
        "cp -a input_files/ drive/My\\ Drive/input_files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLJApAbCoyvW"
      },
      "source": [
        "#List the sorted data\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "sorted(glob.glob(\"./drive/My Drive/Dataset/Dataset/annotation_*bed*.csv\"))\n",
        "sorted(glob.glob(\"./drive/My Drive/Dataset/Dataset/input_*bed*.csv\"))\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klVcfPMecD5s"
      },
      "source": [
        "#Rename shitty data with numbers in the start\n",
        "import glob\n",
        "import os\n",
        "annot_data = sorted(glob.glob(\"./drive/My Drive/Dataset/Dataset/input_161219*.csv\"))\n",
        "for i in range(len(annot_data)):\n",
        "  print(annot_data[i][1:39] + annot_data[i][46:])\n",
        "  os.rename(annot_data[i], annot_data[i][0:39] + annot_data[i][46:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P24Myw8MtgMW"
      },
      "source": [
        "import numpy as np,numpy\n",
        "import csv\n",
        "import glob\n",
        "import os\n",
        "\n",
        "window_size = 1000\n",
        "threshold = 60\n",
        "slide_size = 200 #less than window_size!!!\n",
        "\n",
        "def dataimport(path1, path2):\n",
        "\n",
        "\txx = np.empty([0,window_size,90],float)\n",
        "\tyy = np.empty([0,8],float)\n",
        "\n",
        "\t###Input data###\n",
        "\t#data import from csv\n",
        "\tinput_csv_files = sorted(glob.glob(path1))\n",
        "\tfor f in input_csv_files:\n",
        "\t\tprint(\"input_file_name=\",f)\n",
        "\t\tdata = [[ float(elm) for elm in v] for v in csv.reader(open(f, \"r\"))]\n",
        "\t\ttmp1 = np.array(data)\n",
        "\t\tx2 =np.empty([0,window_size,90],float)\n",
        "\n",
        "\t\t#data import by slide window\n",
        "\t\tk = 0\n",
        "\t\twhile k <= (len(tmp1) + 1 - 2 * window_size):\n",
        "\t\t\tx = np.dstack(np.array(tmp1[k:k+window_size, 1:91]).T)\n",
        "\t\t\tx2 = np.concatenate((x2, x),axis=0)\n",
        "\t\t\tk += slide_size\n",
        "\n",
        "\t\txx = np.concatenate((xx,x2),axis=0)\n",
        "\txx = xx.reshape(len(xx),-1)\n",
        "\n",
        "\t###Annotation data###\n",
        "\t#data import from csv\n",
        "\tannotation_csv_files = sorted(glob.glob(path2))\n",
        "\tfor ff in annotation_csv_files:\n",
        "\t\tprint(\"annotation_file_name=\",ff)\n",
        "\t\tano_data = [[ str(elm) for elm in v] for v in csv.reader(open(ff,\"r\"))]\n",
        "\t\ttmp2 = np.array(ano_data)\n",
        "\n",
        "\t\t#data import by slide window\n",
        "\t\ty = np.zeros(((len(tmp2) + 1 - 2 * window_size)//slide_size+1,8))\n",
        "\t\tk = 0\n",
        "\t\twhile k <= (len(tmp2) + 1 - 2 * window_size):\n",
        "\t\t\ty_pre = np.stack(np.array(tmp2[k:k+window_size]))\n",
        "\t\t\tbed = 0\n",
        "\t\t\tfall = 0\n",
        "\t\t\twalk = 0\n",
        "\t\t\tpickup = 0\n",
        "\t\t\trun = 0\n",
        "\t\t\tsitdown = 0\n",
        "\t\t\tstandup = 0\n",
        "\t\t\tnoactivity = 0\n",
        "\t\t\tfor j in range(window_size):\n",
        "\t\t\t\tif y_pre[j] == \"bed\":\n",
        "\t\t\t\t\tbed += 1\n",
        "\t\t\t\telif y_pre[j] == \"fall\":\n",
        "\t\t\t\t\tfall += 1\n",
        "\t\t\t\telif y_pre[j] == \"walk\":\n",
        "\t\t\t\t\twalk += 1\n",
        "\t\t\t\telif y_pre[j] == \"pickup\":\n",
        "\t\t\t\t\tpickup += 1\n",
        "\t\t\t\telif y_pre[j] == \"run\":\n",
        "\t\t\t\t\trun += 1\n",
        "\t\t\t\telif y_pre[j] == \"sitdown\":\n",
        "\t\t\t\t\tsitdown += 1\n",
        "\t\t\t\telif y_pre[j] == \"standup\":\n",
        "\t\t\t\t\tstandup += 1\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tnoactivity += 1\n",
        "\n",
        "\t\t\tif bed > window_size * threshold / 100:\n",
        "\t\t\t\ty[int(k/slide_size),:] = np.array([0,1,0,0,0,0,0,0])\n",
        "\t\t\telif fall > window_size * threshold / 100:\n",
        "\t\t\t\ty[int(k/slide_size),:] = np.array([0,0,1,0,0,0,0,0])\n",
        "\t\t\telif walk > window_size * threshold / 100:\n",
        "\t\t\t\ty[int(k/slide_size),:] = np.array([0,0,0,1,0,0,0,0])\n",
        "\t\t\telif pickup > window_size * threshold / 100:\n",
        "\t\t\t\ty[int(k/slide_size),:] = np.array([0,0,0,0,1,0,0,0])\n",
        "\t\t\telif run > window_size * threshold / 100:\n",
        "\t\t\t\ty[int(k/slide_size),:] = np.array([0,0,0,0,0,1,0,0])\n",
        "\t\t\telif sitdown > window_size * threshold / 100:\n",
        "\t\t\t\ty[int(k/slide_size),:] = np.array([0,0,0,0,0,0,1,0])\n",
        "\t\t\telif standup > window_size * threshold / 100:\n",
        "\t\t\t\ty[int(k/slide_size),:] = np.array([0,0,0,0,0,0,0,1])\n",
        "\t\t\telse:\n",
        "\t\t\t\ty[int(k/slide_size),:] = np.array([1,0,0,0,0,0,0,0])\n",
        "\t\t\tk += slide_size\n",
        "\n",
        "\t\tyy = np.concatenate((yy, y),axis=0)\n",
        "\tprint(xx.shape,yy.shape)\n",
        "\treturn (xx, yy)\n",
        "\n",
        "\n",
        "#### Main ####\n",
        "if not os.path.exists(\"input_files/\"):\n",
        "        os.makedirs(\"input_files/\")\n",
        "\n",
        "for i, label in enumerate ([\"walk\"]):\n",
        "\tfilepath1 = \"./drive/My Drive/Dataset/Dataset/input_*\" + str(label) + \"*.csv\"\n",
        "\tfilepath2 = \"./drive/My Drive/Dataset/Dataset/annotation_*\" + str(label) + \"*.csv\"\n",
        "\toutputfilename1 = \"./drive/My Drive/input_files/xx_\" + str(window_size) + \"_\" + str(threshold) + \"_\" + label + \".csv\"\n",
        "\toutputfilename2 = \"./drive/My Drive/input_files/yy_\" + str(window_size) + \"_\" + str(threshold) + \"_\" + label + \".csv\"\n",
        "\n",
        "\tx, y = dataimport(filepath1, filepath2)\n",
        "\twith open(outputfilename1, \"w\") as f:\n",
        "\t\twriter = csv.writer(f, lineterminator=\"\\n\")\n",
        "\t\twriter.writerows(x)\n",
        "\twith open(outputfilename2, \"w\") as f:\n",
        "\t\twriter = csv.writer(f, lineterminator=\"\\n\")\n",
        "\t\twriter.writerows(y)\n",
        "\tprint(label + \"finish!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vom7IaS3yB0w"
      },
      "source": [
        "#Number of windows/samples of a particular activity in a csv file\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "file = pd.read_csv(\"./drive/My Drive/input_files/yy_1000_60_bed.csv\")\n",
        "file = np.array(file)\n",
        "np.shape(np.where(file[:,1] == 1))\n",
        "#np.where(file[:,1] == 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMAGGDYhLsvT"
      },
      "source": [
        "#Keeping Only activity Data and Discarding NoActivity\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import glob\n",
        "\n",
        "\n",
        "def csv_write():\n",
        "    window_size = 500\n",
        "    threshold = 60\n",
        "    print(\"csv file importing...\")\n",
        "\n",
        "    for i in [\"standup\"]:\n",
        "#        xx = np.array([[ float(elm) for elm in v] for v in csv.reader(open(\"./input_files/xx_1000_60_\" + str(i) + \".csv\",\"r\"))])\n",
        "#        yy = np.array([[ float(elm) for elm in v] for v in csv.reader(open(\"./input_files/yy_1000_60_\" + str(i) + \".csv\",\"r\"))])\n",
        "\n",
        "#        xx = xx[::2,:]\n",
        "#        yy = yy[::2,:]\n",
        "        label = i\n",
        "        SKIPROW = 2 #Skip every 2 rows -> overlap 800ms to 600ms  (To avoid memory error)\n",
        "        num_lines = sum(1 for l in open(\"./drive/My Drive/input_files/xx_1000_60_\" + str(i) + \".csv\"))\n",
        "        skip_idx = [x for x in range(1, num_lines) if x % SKIPROW !=0]\n",
        "\n",
        "        xx = np.array(pd.read_csv(\"./drive/My Drive/input_files/xx_1000_60_\" + str(i) + \".csv\", header=None, skiprows = skip_idx))\n",
        "        yy = np.array(pd.read_csv(\"./drive/My Drive/input_files/yy_1000_60_\" + str(i) + \".csv\", header=None, skiprows = skip_idx))\n",
        "        print(\"Read Done\")\n",
        "        # eliminate the NoActivity Data\n",
        "        #rows, cols = np.where(yy[:,0] == 1)\n",
        "        rows = np.where(yy[:,0] == 1)\n",
        "        #xx = np.delete(xx, rows[ np.where(cols==0)],0)\n",
        "        xx = np.delete(xx, rows,0)\n",
        "        print(\"Eliminate Done Done\")\n",
        "        #yy = np.delete(yy, rows[ np.where(cols==0)],0)\n",
        "\n",
        "        xx = xx.reshape(len(xx),1000,90)\n",
        "        \n",
        "        # 1000 Hz to 500 Hz (To avoid memory error)\n",
        "        xx = xx[:,::2,:90]\n",
        "\n",
        "        #Rehsape Back to save in CSV\n",
        "        xx = xx.reshape(-1,500*90)\n",
        "        print(\"Downsampling Done\")\n",
        "        print(str(i), \"finished...\", \"xx=\", xx.shape, \"yy=\",  yy.shape)\n",
        "        \n",
        "        #Write Data in CSV Files\n",
        "        outputfilename1 = \"./drive/My Drive/input_files_2/xxx_\" + str(window_size) + \"_\" + str(threshold) + \"_\" + label + \".csv\"\n",
        "        #outputfilename2 = \"./drive/My Drive/input_files_2/yyy_\" + str(window_size) + \"_\" + str(threshold) + \"_\" + label + \".csv\"\n",
        "        with open(outputfilename1, \"w\") as f:\n",
        "          writer = csv.writer(f, lineterminator=\"\\n\")\n",
        "          writer.writerows(xx)\n",
        "        #with open(outputfilename2, \"w\") as f:\n",
        "        #  writer = csv.writer(f, lineterminator=\"\\n\")\n",
        "        #  writer.writerows(y)\n",
        "        print(label + \"finish!\")\n",
        "       \n",
        "      \n",
        "csv_write()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuXCIfzvOEZN"
      },
      "source": [
        "#Keeping Only NoActivity Data and Discarding Activity\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import glob\n",
        "\n",
        "\n",
        "def csv_write_no():\n",
        "    window_size = 500\n",
        "    threshold = 60\n",
        "    print(\"csv file importing...\")\n",
        "\n",
        "    for i in [\"standup\"]:\n",
        "#        xx = np.array([[ float(elm) for elm in v] for v in csv.reader(open(\"./input_files/xx_1000_60_\" + str(i) + \".csv\",\"r\"))])\n",
        "#        yy = np.array([[ float(elm) for elm in v] for v in csv.reader(open(\"./input_files/yy_1000_60_\" + str(i) + \".csv\",\"r\"))])\n",
        "\n",
        "#        xx = xx[::2,:]\n",
        "#        yy = yy[::2,:]\n",
        "        label = i\n",
        "        SKIPROW = 2 #Skip every 2 rows -> overlap 800ms to 600ms  (To avoid memory error)\n",
        "        num_lines = sum(1 for l in open(\"./drive/My Drive/input_files/xx_1000_60_\" + str(i) + \".csv\"))\n",
        "        skip_idx = [x for x in range(1, num_lines) if x % SKIPROW !=0]\n",
        "\n",
        "        xx = np.array(pd.read_csv(\"./drive/My Drive/input_files/xx_1000_60_\" + str(i) + \".csv\", header=None, skiprows = skip_idx))\n",
        "        yy = np.array(pd.read_csv(\"./drive/My Drive/input_files/yy_1000_60_\" + str(i) + \".csv\", header=None, skiprows = skip_idx))\n",
        "        print(\"Read Done\")\n",
        "        # eliminate the NoActivity Data\n",
        "        #rows, cols = np.where(yy[:,0] == 1)\n",
        "        rows = np.where(yy[:,7] == 1)\n",
        "        #xx = np.delete(xx, rows[ np.where(cols==0)],0)\n",
        "        xx = np.delete(xx, rows,0)\n",
        "        print(\"Eliminate Done\")\n",
        "        #yy = np.delete(yy, rows[ np.where(cols==0)],0)\n",
        "\n",
        "        xx = xx.reshape(len(xx),1000,90)\n",
        "        \n",
        "        # 1000 Hz to 500 Hz (To avoid memory error)\n",
        "        xx = xx[:,::2,:90]\n",
        "\n",
        "        #Rehsape Back to save in CSV\n",
        "        xx = xx.reshape(-1,500*90)\n",
        "        print(\"Downsampling Done\")\n",
        "        print(str(i), \"finished...\", \"xx=\", xx.shape, \"yy=\",  yy.shape)\n",
        "        \n",
        "        #Write Data in CSV Files\n",
        "        #outputfilename1 = \"./drive/My Drive/input_files_2/xxx_\" + str(window_size) + \"_\" + str(threshold) + \"_\" + \"none\" + \".csv\"\n",
        "        outputfilename1 = \"./drive/My Drive/Test_Data/xxx_\" + str(window_size) + \"_\" + str(threshold) + \"_\"+ str(i) + \"_\" + \"none\" + \".csv\"\n",
        "        #outputfilename2 = \"./drive/My Drive/input_files_2/yyy_\" + str(window_size) + \"_\" + str(threshold) + \"_\" + label + \".csv\"\n",
        "        with open(outputfilename1, \"w\") as f:\n",
        "          writer = csv.writer(f, lineterminator=\"\\n\")\n",
        "          writer.writerows(xx)\n",
        "        #with open(outputfilename2, \"w\") as f:\n",
        "        #  writer = csv.writer(f, lineterminator=\"\\n\")\n",
        "        #  writer.writerows(y)\n",
        "        print(\"none\" + \"finish!\")\n",
        "        \n",
        "csv_write_no()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOnrrR6QZ4qj"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import glob\n",
        "\n",
        "#Create a NoActivity File from all Other NoActivity Data combined\n",
        "x = np.empty(shape = [0,500,90], dtype = float)\n",
        "\n",
        "for i in [\"bed\", \"fall\", \"pickup\", \"run\", \"sitdown\", \"standup\", \"walk\"]:\n",
        "  print(x.shape)\n",
        "  SKIPROW = 10\n",
        "  num_lines = sum(1 for l in open(\"./drive/My Drive/input_files_2/NoActivity/xxx_500_60_\" + str(i) + \"_none\" +\".csv\"))\n",
        "  skip_idx = [x for x in range(1, num_lines) if x % SKIPROW !=0]\n",
        "  xx = np.array(pd.read_csv(\"./drive/My Drive/input_files_2/NoActivity/xxx_500_60_\" + str(i) + \"_none\" +\".csv\", header=None, skiprows = skip_idx))\n",
        "  print('read ' + str(i) + ' done')\n",
        "  xx = np.reshape(xx,[-1,500,90])\n",
        "  x = np.concatenate((x,xx),axis=0)\n",
        "  print('Cat ' + str(i) + ' done')\n",
        "  \n",
        "x = np.reshape(x,[-1,500*90])\n",
        "\n",
        "  \n",
        "outputfilename1 = \"./drive/My Drive/WifiActivityRecognition/xxx_\" + '500' + \"_\" + '60' + \"_\" + \"none\" + \".csv\"\n",
        "with open(outputfilename1, \"w\") as f:\n",
        "  writer = csv.writer(f, lineterminator=\"\\n\")\n",
        "  writer.writerows(x)\n",
        "  \n",
        "\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iuzSgQ3eJr1"
      },
      "source": [
        "print(x.shape)\n",
        "\n",
        "x = np.reshape(x,[-1,500*90])\n",
        "\n",
        "  \n",
        "outputfilename1 = \"./drive/My Drive/input_files_2/xxx_\" + '500' + \"_\" + '60' + \"_\" + \"none\" + \".csv\"\n",
        "with open(outputfilename1, \"w\") as f:\n",
        "  writer = csv.writer(f, lineterminator=\"\\n\")\n",
        "  writer.writerows(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOWLDrg5L9I3"
      },
      "source": [
        "#Machine Learning Model\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import Dense, GRU, Embedding, LSTM,Dropout\n",
        "from tensorflow.python.keras.optimizers import Adam,SGD,RMSprop\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.python.keras.layers import BatchNormalization\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(200, input_shape=(500,90)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(8,activation = 'softmax'))\n",
        "optimizer = Adam(lr=0.0001)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.summary()\n",
        "model.fit(x, y,\n",
        "          validation_split=0.05, epochs=2000, batch_size=200,shuffle=True)\n",
        "\n",
        "model.save('./drive/My Drive/keras_model/my_model_1_e2000.h5') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQZQ8jP7XAAO"
      },
      "source": [
        "#Importing Data for LSTM\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import csv\n",
        "x = np.empty(shape = [0,500,90], dtype = float)\n",
        "y = np.empty(shape = [0,8], dtype = float)\n",
        "for i in [\"none\",\"bed\", \"fall\", \"pickup\", \"run\", \"sitdown\", \"standup\", \"walk\"]:\n",
        "  f = \"./drive/My Drive/input_files_2/xxx_500_60_\" + str(i) + \".csv\"\n",
        "  print(\"input_file_name=\",f)\n",
        "  data = [[ float(elm) for elm in v] for v in csv.reader(open(f, \"r\"))]\n",
        "  tmp1 = np.array(data)\n",
        "  tmp1 = np.reshape(tmp1,[-1,500,90])\n",
        "  r,c,w = np.shape(tmp1)\n",
        "  x = np.concatenate((x,tmp1),axis = 0)\n",
        "  yy = np.empty([r,8],float)\n",
        "  if i == \"bed\" :\n",
        "     yy[:,:] = np.array([1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0])\n",
        "  elif i == \"fall\":\n",
        "     yy[:,:] = np.array([0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0])\n",
        "  elif i == \"pickup\":\n",
        "     yy[:,:] = np.array([0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0])\n",
        "  elif i == \"run\":\n",
        "     yy[:,:] = np.array([0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0])\n",
        "  elif i == \"sitdown\":\n",
        "     yy[:,:] = np.array([0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0])\n",
        "  elif i == \"standup\":\n",
        "     yy[:,:] = np.array([0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0])\n",
        "  elif i == \"walk\":\n",
        "     yy[:,:] = np.array([0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0])\n",
        "  elif i == \"none\":\n",
        "     yy[:,:] = np.array([0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0])\n",
        "      \n",
        "  y = np.concatenate((y,yy),axis = 0)\n",
        "  \n",
        "  print(np.shape(x))\n",
        "  print(np.shape(y))\n",
        "  print(str(i) + \"done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GheWlrje89Br"
      },
      "source": [
        "print(x.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "so0GBE_05Nys"
      },
      "source": [
        "#Machine Learning Model for TPU\n",
        "%matplotlib inline\n",
        "\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import Dense, GRU, Embedding, LSTM,Dropout\n",
        "from tensorflow.python.keras.optimizers import Adam,SGD,RMSprop\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.python.keras.layers import BatchNormalization\n",
        "from keras import metrics\n",
        "\n",
        "print(keras.__version__)\n",
        "model = Sequential()\n",
        "model.add(LSTM(200, input_shape=(500,90),unit_forget_bias=True,bias_initializer=\"zeros\",return_sequences = False))\n",
        "model.add(Dense(8,activation = 'softmax'))\n",
        "optimizer=tf.train.AdamOptimizer(learning_rate=0.0001)\n",
        "#optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.0001)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "tpu_model = tf.contrib.tpu.keras_to_tpu_model(\n",
        "    model,\n",
        "    strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "        tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))\n",
        "\n",
        "tpu_model.summary()\n",
        "mc = keras.callbacks.ModelCheckpoint('./drive/My Drive/keras_model/weights{epoch:08d}.h5', \n",
        "                                     save_weights_only=True, period=50)\n",
        "\n",
        "history = tpu_model.fit(x, y,\n",
        "                        epochs=2000,\n",
        "                        batch_size=128 * 8,\n",
        "                        validation_split=0.2,shuffle = True,callbacks=[plot,mc])\n",
        "tpu_model.save('./drive/My Drive/keras_model/tpu_model_full_keras212.h5', overwrite=True)\n",
        "#tpu_model.evaluate(x_test, y_test, batch_size=128 * 8)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zp4cltZJt01h"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDqU3YuqbXOH"
      },
      "source": [
        "#Continue to Train model if interrupted\n",
        "\n",
        "#Machine Learning Model for TPU\n",
        "%matplotlib inline\n",
        "\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import Dense, GRU, Embedding, LSTM,Dropout\n",
        "from tensorflow.python.keras.optimizers import Adam,SGD,RMSprop\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.python.keras.layers import BatchNormalization\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(200, input_shape=(500,90),unit_forget_bias=True,bias_initializer=\"zeros\"))\n",
        "model.add(Dense(8,activation = 'softmax'))\n",
        "optimizer=tf.train.AdamOptimizer(learning_rate=0.0001)\n",
        "#optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.0001)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "tpu_model = tf.contrib.tpu.keras_to_tpu_model(\n",
        "    model,\n",
        "    strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "        tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))\n",
        "\n",
        "\n",
        "tpu_model.load_weights('./drive/My Drive/keras_model/tpu_model_full_keras212.h5')\n",
        "tpu_model.summary()\n",
        "\n",
        "history = tpu_model.fit(x, y,\n",
        "                        epochs=2000,\n",
        "                        batch_size=128 * 8,\n",
        "                        validation_split=0.2,shuffle = True,callbacks=[plot])\n",
        "tpu_model.save('./drive/My Drive/keras_model/Final_KerasModel_2000epoch.h5', overwrite=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTMHcr307a6h"
      },
      "source": [
        "tpu_model.save('./drive/My Drive/keras_model/Final_KerasModel_2000epoch.h5', overwrite=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daghJvqNt2NY"
      },
      "source": [
        "#Plot Loss\n",
        "%matplotlib inline\n",
        "\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense, Activation\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "class PlotLosses(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.i = 0\n",
        "        self.x = []\n",
        "        self.losses = []\n",
        "        self.val_losses = []\n",
        "        \n",
        "        self.fig = plt.figure()\n",
        "        \n",
        "        self.logs = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        \n",
        "        self.logs.append(logs)\n",
        "        self.x.append(self.i)\n",
        "        self.losses.append(logs.get('loss'))\n",
        "        self.val_losses.append(logs.get('val_loss'))\n",
        "        self.i += 1\n",
        "        \n",
        "        clear_output(wait=True)\n",
        "        plt.plot(self.x, self.losses, label=\"loss\")\n",
        "        plt.plot(self.x, self.val_losses, label=\"val_loss\")\n",
        "        plt.legend()\n",
        "        plt.show();\n",
        "        \n",
        "plot_losses = PlotLosses()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtQ8AK06zJ3w"
      },
      "source": [
        "#Plot Learning\n",
        "%matplotlib inline\n",
        "\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense, Activation\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "class PlotLearning(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.i = 0\n",
        "        self.x = []\n",
        "        self.losses = []\n",
        "        self.val_losses = []\n",
        "        self.acc = []\n",
        "        self.val_acc = []\n",
        "        self.fig = plt.figure()\n",
        "        \n",
        "        self.logs = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        \n",
        "        self.logs.append(logs)\n",
        "        self.x.append(self.i)\n",
        "        self.losses.append(logs.get('loss'))\n",
        "        self.val_losses.append(logs.get('val_loss'))\n",
        "        self.acc.append(logs.get('acc'))\n",
        "        self.val_acc.append(logs.get('val_acc'))\n",
        "        self.i += 1\n",
        "        f, (ax1, ax2) = plt.subplots(1, 2, sharex=True)\n",
        "        \n",
        "        clear_output(wait=True)\n",
        "        \n",
        "        ax1.set_yscale('log')\n",
        "        ax1.plot(self.x, self.losses, label=\"loss\")\n",
        "        ax1.plot(self.x, self.val_losses, label=\"val_loss\")\n",
        "        ax1.legend()\n",
        "        \n",
        "        ax2.plot(self.x, self.acc, label=\"accuracy\")\n",
        "        ax2.plot(self.x, self.val_acc, label=\"validation accuracy\")\n",
        "        ax2.legend()\n",
        "        \n",
        "        plt.show();\n",
        "        \n",
        "plot = PlotLearning()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M46rensJOnlT"
      },
      "source": [
        "#Machine Learning PRedictor for TPU\n",
        "%matplotlib inline\n",
        "\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import Dense, GRU, Embedding, LSTM,Dropout\n",
        "from tensorflow.python.keras.optimizers import Adam,SGD,RMSprop\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.python.keras.layers import BatchNormalization\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(200, input_shape=(500,90),unit_forget_bias=True,bias_initializer=\"zeros\"))\n",
        "model.add(Dense(8,activation = 'softmax'))\n",
        "optimizer=tf.train.AdamOptimizer(learning_rate=0.0001)\n",
        "#optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.0001)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "tpu_model = tf.contrib.tpu.keras_to_tpu_model(\n",
        "    model,\n",
        "    strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "        tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))\n",
        "\n",
        "\n",
        "tpu_model.load_weights('./drive/My Drive/keras_model/Final_KerasModel_2000epoch.h5')\n",
        "tpu_model.summary()\n",
        "\n",
        "#model_json = model.to_json()\n",
        "#with open(\"./drive/My Drive/keras_model/model_arch.json\", \"w\") as json_file:\n",
        "#    json_file.write(model_json)\n",
        "activity = tpu_model.predict(x_pred)\n",
        "\n",
        "idx = np.argmax(activity,axis = 1);\n",
        "act = [\"bed\", \"fall\", \"pickup\", \"run\", \"sitdown\", \"standup\", \"walk\",\"none\"];\n",
        "for i in idx :\n",
        "  print(act[i])\n",
        "#print(np.argmax(activity,axis = 1))\n",
        "#tpu_model.evaluate(x_test, y_test, batch_size=128 * 8)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byVbR20paNRf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLXZkJlySPJ3"
      },
      "source": [
        "####### Load Prediction/Test Data ############\n",
        "\n",
        "#Importing Data for LSTM\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import csv\n",
        "x_test = np.empty(shape = [0,500,90], dtype = float)\n",
        "for i in [\"none\"]:\n",
        "  #f = \"./drive/My Drive/Test_Data/xxx_500_60_\" + str(i) + \"1.csv\"\n",
        "  f = \"./drive/My Drive/input_files_2/xxx_500_60_\" + str(i) + \".csv\"\n",
        "  print(\"input_file_name=\",f)\n",
        "  data = [[ float(elm) for elm in v] for v in csv.reader(open(f, \"r\"))]\n",
        "  x_pred = np.array(data)\n",
        "  x_pred = np.reshape(x_pred,[-1,500,90])\n",
        "  \n",
        "  #Making it a multiple of 8 for TPU\n",
        "  n=np.shape(x_pred)\n",
        "  n= n[0] - n[0]%8;\n",
        "  x_pred = x_pred[:n,:,:];\n",
        "  \n",
        "  print(np.shape(x_pred))\n",
        "  print(str(i) + \"done\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wV2gQuBkKDvC"
      },
      "source": [
        "x_pred = x_pred[2376::-1,:,:]\n",
        "print(x_pred.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01BGvDnhz4s1"
      },
      "source": [
        " idx = np.argmax(activity,axis = 1);\n",
        "act = [\"bed\", \"fall\", \"pickup\", \"run\", \"sitdown\", \"standup\", \"walk\",\"none\"];\n",
        "for i in idx :\n",
        "  print(act[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieQAW6OHWCrQ"
      },
      "source": [
        "#Machine Learning PRedictor for TPU\n",
        "%matplotlib inline\n",
        "\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import Dense, GRU, Embedding, LSTM,Dropout\n",
        "from tensorflow.python.keras.optimizers import Adam,SGD,RMSprop\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.python.keras.layers import BatchNormalization\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(200, input_shape=(500,90),unit_forget_bias=True,bias_initializer=\"zeros\"))\n",
        "model.add(Dense(8,activation = 'softmax'))\n",
        "optimizer=tf.train.AdamOptimizer(learning_rate=0.0001)\n",
        "#optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.0001)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "tpu_model = tf.contrib.tpu.keras_to_tpu_model(\n",
        "    model,\n",
        "    strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "        tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))\n",
        "\n",
        "\n",
        "tpu_model.load_weights('./drive/My Drive/keras_model/weights00001200.h5')\n",
        "tpu_model.summary()\n",
        "\n",
        "# serialize weights to HDF5\n",
        "#tpu_model.save(\"./drive/My Drive/keras_model/tpu_model_full_keras212.hdf5\")\n",
        "\n",
        "\n",
        "# Save the weights\n",
        "tpu_model.save_weights('./drive/My Drive/keras_model/weights_1200epoch.h5')\n",
        "\n",
        "# Save the model architecture\n",
        "model_json = model.to_json()\n",
        "with open(\"./drive/My Drive/keras_model/model_arch.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "    \n",
        "print(\"Saved model to disk\")\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fH3MBdVXIMP"
      },
      "source": [
        "import socket\n",
        "import sys\n",
        "\n",
        "# Create a TCP/IP socket\n",
        "sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "\n",
        "# Bind the socket to the port\n",
        "server_address = ('0.0.0.0', 8090)\n",
        "print('starting up on %s port %s' % server_address)\n",
        "sock.bind(server_address)\n",
        "\n",
        "\n",
        "# Listen for incoming connections\n",
        "sock.listen(1)\n",
        "\n",
        "\n",
        "while True:\n",
        "    # Wait for a connection\n",
        "    print('waiting for a connection')\n",
        "    connection, client_address = sock.accept()\n",
        "    \n",
        "    \n",
        "    \n",
        "    try:\n",
        "        print('connection from', client_address)\n",
        "\n",
        "        # Receive the data in small chunks and retransmit it\n",
        "        while True:\n",
        "            data = connection.recv(1024)\n",
        "            print('received \"%s\"' % data)\n",
        "            if data:\n",
        "              print('DATA STILL COMING')\n",
        "                #print >>sys.stderr, 'sending data back to the client'\n",
        "                #connection.sendall(data)\n",
        "            else:\n",
        "                print >>sys.stderr, 'no more data from', client_address\n",
        "                break\n",
        "            \n",
        "    finally:\n",
        "        # Clean up the connection\n",
        "        connection.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q841ae3yJtBm"
      },
      "source": [
        "connection.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKM8LoLnfk3f"
      },
      "source": [
        "import keras; \n",
        "print(keras.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kaG1KBQR_ur"
      },
      "source": [
        "!pip install keras==2.1.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCmWYtuFRC9-"
      },
      "source": [
        "#Testing Matrices\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "SKIPROW = 2 \n",
        "num_lines = sum(1 for l in open(\"./drive/My Drive/input_files/xx_1000_60_\" + \"run\" + \".csv\"))\n",
        "skip_idx = [x for x in range(1, num_lines) if x % SKIPROW !=0]\n",
        "\n",
        "x_run = np.array(pd.read_csv(\"./drive/My Drive/input_files/xx_1000_60_\" + \"run\" + \".csv\", header=None, skiprows = skip_idx))\n",
        "x_no = np.array(pd.read_csv(\"./drive/My Drive/Test_Data/xxx_500_60_\" + \"run\" + \"1.csv\", header=None))\n",
        "print(x_run.shape)\n",
        "print(x_no.shape)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBzjwjozM6kC"
      },
      "source": [
        "x_no = np.array(pd.read_csv(\"./drive/My Drive/Test_Data/xxx_500_60_\" + \"none\" + \"1.csv\", header=None))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QCeQlFMSUdP"
      },
      "source": [
        "print(x_run.shape)\n",
        "print(x_no.shape)\n",
        "\n",
        "print(x_run[3590-35:3590,1])\n",
        "print(x_no[2381-10:2381,1])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcoexTKYVBdW"
      },
      "source": [
        "x_run = x_run.reshape(len(x_run),1000,90)\n",
        "x_run = x_run[:,::2,:90]\n",
        "print(x_run.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oj9wdoEi2yhU"
      },
      "source": [
        "x_run = x_run[:3584,:,:]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}